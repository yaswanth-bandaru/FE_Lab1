{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First of all we'll load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = r\"Y:\\Masters_Content\\Feature_Engineering\\Lab1\\Codes\\News_dataset.pickle\"\n",
    "\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>id</th>\n",
       "      <th>News_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Tempe Police Department said it was invest...</td>\n",
       "      <td>We continue to assist investigators in any way...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>1</td>\n",
       "      <td>799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On Sunday, the inevitable happened: An autonom...</td>\n",
       "      <td>Cars don’t see wellAutonomous cars don’t track...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>1</td>\n",
       "      <td>743.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Even before a driverless Uber vehicle struck a...</td>\n",
       "      <td>The accident in Tempe, Arizona, was believed t...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>1</td>\n",
       "      <td>587.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Sunday night, a woman died after she was hi...</td>\n",
       "      <td>On Sunday night, a woman died after she was hi...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>1</td>\n",
       "      <td>497.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A self-driving vehicle made by Uber has struck...</td>\n",
       "      <td>Something unexpectedly entering the vehicle’s ...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>1</td>\n",
       "      <td>728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>507.txt</td>\n",
       "      <td>Big guns ease through in San Jose\\r\\n\\r\\nTop-s...</td>\n",
       "      <td>sport</td>\n",
       "      <td>1</td>\n",
       "      <td>1376.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>508.txt</td>\n",
       "      <td>Almagro continues Spanish surge\\r\\n\\r\\nUnseede...</td>\n",
       "      <td>sport</td>\n",
       "      <td>1</td>\n",
       "      <td>779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>509.txt</td>\n",
       "      <td>Melzer shocks Agassi in San Jose\\r\\n\\r\\nSecond...</td>\n",
       "      <td>sport</td>\n",
       "      <td>1</td>\n",
       "      <td>1154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>510.txt</td>\n",
       "      <td>Mirza makes Indian tennis history\\r\\n\\r\\nTeena...</td>\n",
       "      <td>sport</td>\n",
       "      <td>1</td>\n",
       "      <td>1779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>511.txt</td>\n",
       "      <td>Roddick to face Saulnier in final\\r\\n\\r\\nAndy ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>1</td>\n",
       "      <td>1177.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2099 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              File_Name  \\\n",
       "0     The Tempe Police Department said it was invest...   \n",
       "1     On Sunday, the inevitable happened: An autonom...   \n",
       "2     Even before a driverless Uber vehicle struck a...   \n",
       "3     On Sunday night, a woman died after she was hi...   \n",
       "4     A self-driving vehicle made by Uber has struck...   \n",
       "...                                                 ...   \n",
       "2094                                            507.txt   \n",
       "2095                                            508.txt   \n",
       "2096                                            509.txt   \n",
       "2097                                            510.txt   \n",
       "2098                                            511.txt   \n",
       "\n",
       "                                                Content        Category  id  \\\n",
       "0     We continue to assist investigators in any way...  autonomous car   1   \n",
       "1     Cars don’t see wellAutonomous cars don’t track...  autonomous car   1   \n",
       "2     The accident in Tempe, Arizona, was believed t...  autonomous car   1   \n",
       "3     On Sunday night, a woman died after she was hi...  autonomous car   1   \n",
       "4     Something unexpectedly entering the vehicle’s ...  autonomous car   1   \n",
       "...                                                 ...             ...  ..   \n",
       "2094  Big guns ease through in San Jose\\r\\n\\r\\nTop-s...           sport   1   \n",
       "2095  Almagro continues Spanish surge\\r\\n\\r\\nUnseede...           sport   1   \n",
       "2096  Melzer shocks Agassi in San Jose\\r\\n\\r\\nSecond...           sport   1   \n",
       "2097  Mirza makes Indian tennis history\\r\\n\\r\\nTeena...           sport   1   \n",
       "2098  Roddick to face Saulnier in final\\r\\n\\r\\nAndy ...           sport   1   \n",
       "\n",
       "      News_length  \n",
       "0           799.0  \n",
       "1           743.0  \n",
       "2           587.0  \n",
       "3           497.0  \n",
       "4           728.0  \n",
       "...           ...  \n",
       "2094       1376.0  \n",
       "2095        779.0  \n",
       "2096       1154.0  \n",
       "2097       1779.0  \n",
       "2098       1177.0  \n",
       "\n",
       "[2099 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### And visualize one sample news content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cars don’t see wellAutonomous cars don’t track the center line of the street well on ill-maintained roads.\\nAt their worst, autonomous cars might be murder machinesMany people claim that autonomous cars could save lives.\\nBut an overwhelming number of tech people (and investors) seem to want self-driving cars so badly that they are willing to ignore evidence suggesting that self-driving cars could cause as much harm as good.\\n**Nobody needs a self-driving car to avoid trafficPlenty of people want self-driving cars to make their lives easier, but self-driving cars aren’t the only way to fix America’s traffic problems.\\nPeople are warning of a likely future for self-driving cars that is neither safe nor ethical nor toward the greater good.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1]['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Special character cleaning\n",
    "\n",
    "##### We can see the following special characters:\n",
    "\n",
    "##### \\r\n",
    "##### \\n\n",
    "##### \\ before possessive pronouns (government's = government\\'s)\n",
    "##### \\ before possessive pronouns 2 (Yukos' = Yukos\\')\n",
    "##### \" when quoting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\r and \\n\n",
    "df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mr Greenspan's\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Mr Greenspan\\'s\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" when quoting text\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Upcase/downcase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll downcase the texts because we want, for example, Football and football to be the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the text\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Punctuation signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Punctuation signs won't have any predicting power, so we'll just get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_signs = list(\"?:!.,;\")\n",
    "df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Possessive pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll also remove possessive pronoun terminations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since stemming can produce output words that don't exist, we'll only use a lemmatization process at this moment. Lemmatization takes into consideration the morphological analysis of the words and returns words that do exist, so it will be more useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In order to lemmatize, we have to iterate through every word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    \n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.loc[row]['Content_Parsed_4']\n",
    "    text_words = text.split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_5'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Although lemmatization doesn't work perfectly in all cases (as can be seen in the example below), it can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading the stop words list\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To remove the stop words, we'll handle a regular expression only detecting whole words, as seen in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'StopWord eating a meal'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"me eating a meal\"\n",
    "word = \"me\"\n",
    "\n",
    "# The regular expression is:\n",
    "regex = r\"\\b\" + word + r\"\\b\"  # we need to build it like that to work properly\n",
    "\n",
    "re.sub(regex, \"StopWord\", example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can now loop through all the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "\n",
    "for stop_word in stop_words:\n",
    "\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  We have some dobule/triple spaces between words because of the replacements. However, it's not a problem because we'll tokenize by the spaces later.\n",
    "\n",
    "##### As an example, we'll show an original news article and its modifications throughout the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An autonomous Uber car killed a woman in the street in Arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the US.\\nThe company said it was pausing its self-driving car operations in Phoenix, Pittsburgh, San Francisco and Toronto.\\nPhotograph: APThe self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes.\\nSimpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian.\\nEarlier this year, California regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Special character cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An autonomous Uber car killed a woman in the street in Arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the US. The company said it was pausing its self-driving car operations in Phoenix, Pittsburgh, San Francisco and Toronto. Photograph: APThe self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes. Simpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian. Earlier this year, California regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Upcase/downcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an autonomous uber car killed a woman in the street in arizona, police said, in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the us. the company said it was pausing its self-driving car operations in phoenix, pittsburgh, san francisco and toronto. photograph: apthe self-driving technology is supposed to detect pedestrians, cyclists and others and prevent crashes. simpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian. earlier this year, california regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Punctuation signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an autonomous uber car killed a woman in the street in arizona police said in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the us the company said it was pausing its self-driving car operations in phoenix pittsburgh san francisco and toronto photograph apthe self-driving technology is supposed to detect pedestrians cyclists and others and prevent crashes simpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian earlier this year california regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Possessive pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an autonomous uber car killed a woman in the street in arizona police said in what appears to be the first reported fatal crash involving a self-driving vehicle and a pedestrian in the us the company said it was pausing its self-driving car operations in phoenix pittsburgh san francisco and toronto photograph apthe self-driving technology is supposed to detect pedestrians cyclists and others and prevent crashes simpson said he was unaware of any previous fatal crashes involving an autonomous vehicle and a pedestrian earlier this year california regulators approved the testing of self-driving cars on public roads without human drivers monitoring inside'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an autonomous uber car kill a woman in the street in arizona police say in what appear to be the first report fatal crash involve a self-driving vehicle and a pedestrian in the us the company say it be pause its self-driving car operations in phoenix pittsburgh san francisco and toronto photograph apthe self-driving technology be suppose to detect pedestrians cyclists and others and prevent crash simpson say he be unaware of any previous fatal crash involve an autonomous vehicle and a pedestrian earlier this year california regulators approve the test of self-driving cars on public roads without human drivers monitor inside'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' autonomous uber car kill  woman   street  arizona police say   appear    first report fatal crash involve  self-driving vehicle   pedestrian   us  company say   pause  self-driving car operations  phoenix pittsburgh san francisco  toronto photograph apthe self-driving technology  suppose  detect pedestrians cyclists  others  prevent crash simpson say   unaware   previous fatal crash involve  autonomous vehicle   pedestrian earlier  year california regulators approve  test  self-driving cars  public roads without human drivers monitor inside'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[5]['Content_Parsed_6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, we can delete the intermediate columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Content</th>\n",
       "      <th>Category</th>\n",
       "      <th>id</th>\n",
       "      <th>News_length</th>\n",
       "      <th>Content_Parsed_1</th>\n",
       "      <th>Content_Parsed_2</th>\n",
       "      <th>Content_Parsed_3</th>\n",
       "      <th>Content_Parsed_4</th>\n",
       "      <th>Content_Parsed_5</th>\n",
       "      <th>Content_Parsed_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Tempe Police Department said it was invest...</td>\n",
       "      <td>We continue to assist investigators in any way...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>1</td>\n",
       "      <td>799.0</td>\n",
       "      <td>We continue to assist investigators in any way...</td>\n",
       "      <td>we continue to assist investigators in any way...</td>\n",
       "      <td>we continue to assist investigators in any way...</td>\n",
       "      <td>we continue to assist investigators in any way...</td>\n",
       "      <td>we continue to assist investigators in any way...</td>\n",
       "      <td>continue  assist investigators   way  ”uber  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File_Name  \\\n",
       "0  The Tempe Police Department said it was invest...   \n",
       "\n",
       "                                             Content        Category  id  \\\n",
       "0  We continue to assist investigators in any way...  autonomous car   1   \n",
       "\n",
       "   News_length                                   Content_Parsed_1  \\\n",
       "0        799.0  We continue to assist investigators in any way...   \n",
       "\n",
       "                                    Content_Parsed_2  \\\n",
       "0  we continue to assist investigators in any way...   \n",
       "\n",
       "                                    Content_Parsed_3  \\\n",
       "0  we continue to assist investigators in any way...   \n",
       "\n",
       "                                    Content_Parsed_4  \\\n",
       "0  we continue to assist investigators in any way...   \n",
       "\n",
       "                                    Content_Parsed_5  \\\n",
       "0  we continue to assist investigators in any way...   \n",
       "\n",
       "                                    Content_Parsed_6  \n",
       "0   continue  assist investigators   way  ”uber  ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Complete_Filename'] = df['File_Name'] + '-' + df['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = [\"File_Name\", \"Category\", \"Complete_Filename\", \"Content\", \"Content_Parsed_6\"]\n",
    "df = df[list_columns]\n",
    "\n",
    "df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complete_Filename</th>\n",
       "      <th>Content</th>\n",
       "      <th>Content_Parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Tempe Police Department said it was invest...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>The Tempe Police Department said it was invest...</td>\n",
       "      <td>We continue to assist investigators in any way...</td>\n",
       "      <td>continue  assist investigators   way  ”uber  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On Sunday, the inevitable happened: An autonom...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>On Sunday, the inevitable happened: An autonom...</td>\n",
       "      <td>Cars don’t see wellAutonomous cars don’t track...</td>\n",
       "      <td>cars ’ see wellautonomous cars ’ track  center...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Even before a driverless Uber vehicle struck a...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>Even before a driverless Uber vehicle struck a...</td>\n",
       "      <td>The accident in Tempe, Arizona, was believed t...</td>\n",
       "      <td>accident  tempe arizona  believe    first tim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Sunday night, a woman died after she was hi...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>On Sunday night, a woman died after she was hi...</td>\n",
       "      <td>On Sunday night, a woman died after she was hi...</td>\n",
       "      <td>sunday night  woman die    hit   self-driving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A self-driving vehicle made by Uber has struck...</td>\n",
       "      <td>autonomous car</td>\n",
       "      <td>A self-driving vehicle made by Uber has struck...</td>\n",
       "      <td>Something unexpectedly entering the vehicle’s ...</td>\n",
       "      <td>something unexpectedly enter  vehicle’ path  p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File_Name        Category  \\\n",
       "0  The Tempe Police Department said it was invest...  autonomous car   \n",
       "1  On Sunday, the inevitable happened: An autonom...  autonomous car   \n",
       "2  Even before a driverless Uber vehicle struck a...  autonomous car   \n",
       "3  On Sunday night, a woman died after she was hi...  autonomous car   \n",
       "4  A self-driving vehicle made by Uber has struck...  autonomous car   \n",
       "\n",
       "                                   Complete_Filename  \\\n",
       "0  The Tempe Police Department said it was invest...   \n",
       "1  On Sunday, the inevitable happened: An autonom...   \n",
       "2  Even before a driverless Uber vehicle struck a...   \n",
       "3  On Sunday night, a woman died after she was hi...   \n",
       "4  A self-driving vehicle made by Uber has struck...   \n",
       "\n",
       "                                             Content  \\\n",
       "0  We continue to assist investigators in any way...   \n",
       "1  Cars don’t see wellAutonomous cars don’t track...   \n",
       "2  The accident in Tempe, Arizona, was believed t...   \n",
       "3  On Sunday night, a woman died after she was hi...   \n",
       "4  Something unexpectedly entering the vehicle’s ...   \n",
       "\n",
       "                                      Content_Parsed  \n",
       "0   continue  assist investigators   way  ”uber  ...  \n",
       "1  cars ’ see wellautonomous cars ’ track  center...  \n",
       "2   accident  tempe arizona  believe    first tim...  \n",
       "3   sunday night  woman die    hit   self-driving...  \n",
       "4  something unexpectedly enter  vehicle’ path  p...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Label coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We'll create a dictionary with the label codification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'autonomous car': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category mapping\n",
    "df['Category_Code'] = df['Category']\n",
    "df = df.replace({'Category_Code':category_codes})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
    "                                                    df['Category_Code'], \n",
    "                                                    test_size=0.15, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since we don't have much observations (only 2.225), we'll choose a test set size of 15% of the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have chosen these values as a first approximation. Since the models that we develop later have a very good predictive power, we'll stick to these values. But it has to be mentioned that different combinations could be tried in order to improve even more the accuracy of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1782, 300)\n",
      "(315, 300)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can use the Chi squared test in order to see what unigrams and bigrams are most correlated with each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'autonomous car' category:\n",
      "  . Most correlated unigrams:\n",
      ". vehicles\n",
      ". self\n",
      ". cars\n",
      ". driving\n",
      ". autonomous\n",
      "  . Most correlated bigrams:\n",
      ". year old\n",
      ". self driving\n",
      "\n",
      "# 'business' category:\n",
      "  . Most correlated unigrams:\n",
      ". oil\n",
      ". market\n",
      ". bank\n",
      ". growth\n",
      ". firm\n",
      "  . Most correlated bigrams:\n",
      ". year old\n",
      ". self driving\n",
      "\n",
      "# 'entertainment' category:\n",
      "  . Most correlated unigrams:\n",
      ". tv\n",
      ". music\n",
      ". star\n",
      ". award\n",
      ". film\n",
      "  . Most correlated bigrams:\n",
      ". mr blair\n",
      ". self driving\n",
      "\n",
      "# 'politics' category:\n",
      "  . Most correlated unigrams:\n",
      ". tory\n",
      ". blair\n",
      ". election\n",
      ". party\n",
      ". labour\n",
      "  . Most correlated bigrams:\n",
      ". prime minister\n",
      ". mr blair\n",
      "\n",
      "# 'sport' category:\n",
      "  . Most correlated unigrams:\n",
      ". champion\n",
      ". coach\n",
      ". cup\n",
      ". match\n",
      ". game\n",
      "  . Most correlated bigrams:\n",
      ". self driving\n",
      ". year old\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['last year',\n",
       " 'tell bbc',\n",
       " 'mr brown',\n",
       " 'prime minister',\n",
       " 'mr blair',\n",
       " 'self driving',\n",
       " 'year old']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see there are only six. This means the unigrams have more correlation with the category than the bigrams, and since we're restricting the number of features to the most representative 300, only a few bigrams are being considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's save the files we'll need in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "with open('Pickles/X_train.pickle', 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "with open('Pickles/X_test.pickle', 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "with open('Pickles/y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "with open('Pickles/y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "with open('Pickles/df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)\n",
    "    \n",
    "# features_train\n",
    "with open('Pickles/features_train.pickle', 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "with open('Pickles/labels_train.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "with open('Pickles/features_test.pickle', 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('Pickles/labels_test.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "with open('Pickles/tfidf.pickle', 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
